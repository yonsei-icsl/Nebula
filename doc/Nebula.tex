\documentclass[10pt]{article}

\usepackage[letterpaper,margin=1.0in]{geometry}
\usepackage{array}
\usepackage{fancyvrb}
\usepackage{indentfirst}
\usepackage[hidelinks]{hyperref}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{secdot}
\usepackage{sectsty}
\usepackage{times}
\usepackage{subfigure}
\usepackage{verbatimbox}
\usepackage[dvipsnames]{xcolor}
\sectionfont{\fontsize{12pt}{12pt}\selectfont}

\begin{document}
\title{Nebula: Lightweight Neural Network Benchmarks}
\author{Bogil Kim, Sungjae Lee, Chanho Park, Hyeonjin Kim, and William J. Song\\
        School of Electrical Engineering, Yonsei University\\}
\date{Nov. 2020 (Version 1.4)}
\maketitle

\section{Introduction} \label{sec:introduction}
\emph{Nebula} is a lightweight benchmark suite for neural networks. 
Recent neural networks become increasingly deeper and larger. 
This trend puts great challenges on the modeling and analysis of computer systems in that it takes longer execution time to process a large number of operations and sizable data. 
Nebula tackles this challenge by taking the opposite direction based on an observation that the computations of neural networks are mainly comprised of matrix and vector operations. 
Nebula benchmarks drastically reduce the number of operations for an expectation that the lightened networks will still have similar architectural behaviors as the full-fledged neural networks, and thus it is not always necessary to run the complete networks with sizable data if the purpose is to characterize micro-architectural behaviors in computer systems. 
Results based on hardware measurements prove that Nebula benchmarks indeed meet the similarity and affordability goal.

\section{Prerequisite, Download, and Installation} \label{sec:install}
The Nebula benchmark suite implements a C++ framework that facilitates the composition of diverse neural networks, similar to the well-known Caffe framework.
The framework standardizes the implementation of network and layer classes for easier extension to new type of neural networks.
The standardization makes it convenient to interface with them and share acceleration libraries among the various implementations of neural networks.

Nebula requires g++ compiler to build on CPU, nvcc on GPU, and OpenCV for image processing.
It utilizes several acceleration libraries encompassing OpenBLAS for CPU and CUDA libraries (e.g., cuBLAS and cuDNN) to speed up the computations of neural networks, but the implementations are not limited to those external libraries.
To install prerequisite to run the Nebula benchmark, use the following command in terminal.

\begin{Verbatim}[frame=single,fontsize=\small]
[required] * g++
[required] * OpenCV
$ sudo apt-get install libopencv-dev
[optional] * nvcc
[optional] * cuBLAS for GPU 
[optional] * cuDNN for GPU
[optional] * OpenBLAS for CPU
$ sudo apt-get install libopenblas-dev
\end{Verbatim}

It has been validated in Ubuntu 16.04 and 18.04.
To obtain a copy of Nebula v1.0 (as of July, 2020), use the following command in terminal.
Then, enter the \emph{nebula/} directory and build the benchmark using shell script \emph{nebula.sh}.

\begin{Verbatim}[frame=single,fontsize=\small]
$ git clone --branch v1.0 https://github.com/yonsei-icsl/nebula
$ cd nebula/
$ ./nebula.sh build all
\end{Verbatim}

After downloading the Nebula framework, you can build Nebula with various options in shell script \emph{nebula.sh}.

\begin{Verbatim}[frame=single,fontsize=\small]
# nebula.sh
##### Optional arguments #####
# Use gdb for debug.
  use_debug=0
# Use GPU.
  gpu_enabled=1
# Use custom blas.
  custom_blas=0
# Do not load weight for training by default.
  load_weight=0
\end{Verbatim}
If you want to build specific network, build the benchmark using following commands.
\begin{Verbatim}[frame=single,fontsize=\small]
$ ./nebula.sh build lib
$ ./nebula.sh build <benchmark>
\end{Verbatim}
The Nebula suite consists of seven representative neural networks including AlexNet, VGG, ResNet, MLP, DBN, RNN, and LSTM.
Each network offers three size options; Large, Medium, and Small.
All benchmarks and size should be type in lowercase.
For example, if you want to build Small version of VGG, use following command.

\begin{Verbatim}[frame=single,fontsize=\small]
$ ./nebula.sh build vgg_S
\end{Verbatim}

\section{Running Nebula Framework} \label{sec:running}

Nebula runs the neural network with network configuration file, dataset image, and weight file.
Network configuration file is located in benchmark directory (\emph{benchmarks/vgg\_S/network.cfg}).
You can obtain dataset image and weight file using the command \emph{get\_data.sh} and \emph{get\_weight.sh}, respectively.

\subsection{Preparing Database} \label{subsec:database}
You can download the dataset folder using the command \emph{get\_data.sh}.
Nebula includes ImageNet for convolutional networks, NIST for fully connected networks, and PTB for recurrent networks.
And each dataset supports variable-sized options from large to small datasets for various types of neural networks. 
Following shows the example of downloading small size of ImageNet.

\begin{Verbatim}[frame=single,fontsize=\small]
$ ./get_data.sh
Which dataset? [ImageNet[I] / NIST[N] / MNIST[M]] I
Which size? [Large[L] / Medium[M] / Small[S]] S
Get data ID of ImageNet_S
--2020-04-16 19:11:14--  https://docs.google.com/uc?export=download&confirm=2bpp&id=1z
CTB82sCmBcf0tCapOa0cd7tAcPp9ADe
Resolving docs.google.com (docs.google.com)... 172.217.25.206, 2404:6800:4004:81a::200
e
Connecting to docs.google.com (docs.google.com)|172.217.25.206|:443... connected.
rk] train network.cfg data.cfg input.wgh(optional) output.wgh(optional)
 41 

HTTP request sent, awaiting response... 302 Moved Temporarily

...

Connecting to doc-0o-a0-docs.googleusercontent.com (doc-0o-a0-docs.googleusercontent.c
om)|172.217.161.33|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: unspecified [application/x-tar]
Saving to: ‘ImageNet_S.tar’

ImageNet_S.tar               [               <=>               ]   2.82G  5.15MB/s  

\end{Verbatim}

After downloading the dataset, you should copy dataset list file to the benchmark folder.
Dataset list includes \emph{train.lst}, \emph{test.lst}, and \emph{labels.lst}.
\emph{train.lst} and \emph{test.lst} contain the path of data file for train and test, and \emph{labels.lst} contains the label data of dataset.

\begin{Verbatim}[frame=single,fontsize=\small]
$ cd nebula/
$ cp data/ImageNet_S/*.lst benchmarks/vgg_S/
\end{Verbatim}

\subsection{Preparing Weight for Inference} \label{subsec:weight}
When you want to run the neural network in inference phase, you need a weight file.
You can download weight file using the shell script \emph{get\_weight.sh}.
The weight file is downloaded as \emph{input.wgh} at each benchmark directory.
Following shows the example of downloading weight file of Small VGG.

\begin{Verbatim}[frame=single,fontsize=\small]

$ ./get_weight.sh vgg_S

Get file ID of vgg_S
/home/bogil/Publication/nebula/benchmarks/vgg_S
--2020-04-16 19:23:32--  https://docs.google.com/uc?export=download&confirm=&id=1L6GzG
0Je43jd6sVWICFC8oCVEtP507ee
Resolving docs.google.com (docs.google.com)... 172.217.25.206, 2404:6800:4004:81a::200
e
Connecting to docs.google.com (docs.google.com)|172.217.25.206|:443... connected.
HTTP request sent, awaiting response... 302 Moved Temporarily

...

Connecting to doc-00-2g-docs.googleusercontent.com (doc-00-2g-docs.googleusercontent.c
om)|172.217.161.33|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: unspecified [application/octet-stream]
Saving to: ‘input.wgh’

input.wgh                      [           <=>                  ]  11.74M  5.31MB/s

\end{Verbatim}

\subsection{Running the Neural Network} \label{subsec:running_network}

With input files (i.e., network configuration file, dataset image, and weight file of network), you can run variable-sized neural networks.
After executing the network, Nebula prints out the result of execution depending on the execution model (i.e., inference and training).
The result of inference includes instantaneous accuracy, cumulative accuracy, and execution time.
On the other hands, an instantaneous loss, and a cumulative loss, and runtime are printed out at the end of every iteration of training phase.
\begin{Verbatim}[frame=single,fontsize=\small]
$ ./nebula.sh test vgg_S
 _____________________________________________________________
|                                                             |
|    * *       #   #  #####  ###     #   #  #      ##     *   |
|        *    ##  #  #      #   #   #   #  #      # #   *   * |
|     *      # # #  #####  #####   #   #  #      ####     *   |
|   *       #  ##  #      #    #  #   #  #      #   #      *  |
|      *   #   #  #####  ######  #####  #####  #    #    *    |
|                                                             |
|                                                             |
|                                                             |
|  Nebula: Lightweight Neural Network Benchmarks              |
|  Intelligent Computing System Lab (ICSL)                    |
|  School of Electrical Engineering, Yonsei University        |
|  Version: 1.4                                               |
|_____________________________________________________________|
Initializing network ...
Running network ...
Iteration #0 (data #32):
  - accuracy: 50.000000%
  - runtime: 49.327999msec
Iteration #1 (data #64):
  - accuracy: 53.125000%
  - runtime: 49.601002msec
Iteration #2 (data #96):
  - accuracy: 53.125000%
  - runtime: 50.285999msec
Iteration #3 (data #128):
  - accuracy: 52.343750%
  - runtime: 49.383999msec
Iteration #4 (data #160):
  - accuracy: 57.500000%
  - runtime: 50.063000msec

...

Total runtime: 3.677841sec

Network test done.
\end{Verbatim}

\section{Contact} \label{sec:contact}
In case you notice a bug or have a question regarding the use of Nebula benchmark suite, please feel free to contact via email, \href{mailto:bogilkim@yonsei.ac.kr}{bogilkim@yonsei.ac.kr}.

\end{document}
